{
    "model_name": "ISTA-DASLab/Meta-Llama-3-8B-AQLM-PV-2Bit-1x16",
    "config": {
        "model": "hf",
        "model_args": "pretrained=ISTA-DASLab/Meta-Llama-3-8B-AQLM-PV-2Bit-1x16",
        "model_num_parameters": 2042171392,
        "model_dtype": "torch.float16",
        "model_revision": "main",
        "model_sha": "ad1d994ac589738b729ece3c334edd6765bed365",
        "batch_size": "16",
        "batch_sizes": [],
        "device": null,
        "use_cache": null,
        "limit": null,
        "bootstrap_iters": 100000,
        "gen_kwargs": null,
        "random_seed": 0,
        "numpy_seed": 1234,
        "torch_seed": 1234,
        "fewshot_seed": 1234
    },
    "configs": {
        "mmlu_abstract_algebra": {
            "task": "mmlu_abstract_algebra",
            "task_alias": "abstract_algebra",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "abstract_algebra",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_anatomy": {
            "task": "mmlu_anatomy",
            "task_alias": "anatomy",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "anatomy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_astronomy": {
            "task": "mmlu_astronomy",
            "task_alias": "astronomy",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "astronomy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_business_ethics": {
            "task": "mmlu_business_ethics",
            "task_alias": "business_ethics",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "business_ethics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_clinical_knowledge": {
            "task": "mmlu_clinical_knowledge",
            "task_alias": "clinical_knowledge",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "clinical_knowledge",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_college_biology": {
            "task": "mmlu_college_biology",
            "task_alias": "college_biology",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_biology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_college_chemistry": {
            "task": "mmlu_college_chemistry",
            "task_alias": "college_chemistry",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_chemistry",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_college_computer_science": {
            "task": "mmlu_college_computer_science",
            "task_alias": "college_computer_science",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_computer_science",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_college_mathematics": {
            "task": "mmlu_college_mathematics",
            "task_alias": "college_mathematics",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_mathematics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_college_medicine": {
            "task": "mmlu_college_medicine",
            "task_alias": "college_medicine",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_college_physics": {
            "task": "mmlu_college_physics",
            "task_alias": "college_physics",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_physics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_computer_security": {
            "task": "mmlu_computer_security",
            "task_alias": "computer_security",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "computer_security",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_conceptual_physics": {
            "task": "mmlu_conceptual_physics",
            "task_alias": "conceptual_physics",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "conceptual_physics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_econometrics": {
            "task": "mmlu_econometrics",
            "task_alias": "econometrics",
            "group": "mmlu_social_sciences",
            "group_alias": "social_sciences",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "econometrics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_electrical_engineering": {
            "task": "mmlu_electrical_engineering",
            "task_alias": "electrical_engineering",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "electrical_engineering",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_elementary_mathematics": {
            "task": "mmlu_elementary_mathematics",
            "task_alias": "elementary_mathematics",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "elementary_mathematics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_formal_logic": {
            "task": "mmlu_formal_logic",
            "task_alias": "formal_logic",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "formal_logic",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_global_facts": {
            "task": "mmlu_global_facts",
            "task_alias": "global_facts",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "global_facts",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_biology": {
            "task": "mmlu_high_school_biology",
            "task_alias": "high_school_biology",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_biology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_chemistry": {
            "task": "mmlu_high_school_chemistry",
            "task_alias": "high_school_chemistry",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_chemistry",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_computer_science": {
            "task": "mmlu_high_school_computer_science",
            "task_alias": "high_school_computer_science",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_computer_science",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_european_history": {
            "task": "mmlu_high_school_european_history",
            "task_alias": "high_school_european_history",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_european_history",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_geography": {
            "task": "mmlu_high_school_geography",
            "task_alias": "high_school_geography",
            "group": "mmlu_social_sciences",
            "group_alias": "social_sciences",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_geography",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_government_and_politics": {
            "task": "mmlu_high_school_government_and_politics",
            "task_alias": "high_school_government_and_politics",
            "group": "mmlu_social_sciences",
            "group_alias": "social_sciences",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_government_and_politics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_macroeconomics": {
            "task": "mmlu_high_school_macroeconomics",
            "task_alias": "high_school_macroeconomics",
            "group": "mmlu_social_sciences",
            "group_alias": "social_sciences",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_macroeconomics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_mathematics": {
            "task": "mmlu_high_school_mathematics",
            "task_alias": "high_school_mathematics",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_mathematics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_microeconomics": {
            "task": "mmlu_high_school_microeconomics",
            "task_alias": "high_school_microeconomics",
            "group": "mmlu_social_sciences",
            "group_alias": "social_sciences",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_microeconomics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_physics": {
            "task": "mmlu_high_school_physics",
            "task_alias": "high_school_physics",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_physics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_psychology": {
            "task": "mmlu_high_school_psychology",
            "task_alias": "high_school_psychology",
            "group": "mmlu_social_sciences",
            "group_alias": "social_sciences",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_psychology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_statistics": {
            "task": "mmlu_high_school_statistics",
            "task_alias": "high_school_statistics",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_statistics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_us_history": {
            "task": "mmlu_high_school_us_history",
            "task_alias": "high_school_us_history",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_us_history",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_high_school_world_history": {
            "task": "mmlu_high_school_world_history",
            "task_alias": "high_school_world_history",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_world_history",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_human_aging": {
            "task": "mmlu_human_aging",
            "task_alias": "human_aging",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "human_aging",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_human_sexuality": {
            "task": "mmlu_human_sexuality",
            "task_alias": "human_sexuality",
            "group": "mmlu_social_sciences",
            "group_alias": "social_sciences",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "human_sexuality",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_international_law": {
            "task": "mmlu_international_law",
            "task_alias": "international_law",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "international_law",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about international law.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_jurisprudence": {
            "task": "mmlu_jurisprudence",
            "task_alias": "jurisprudence",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "jurisprudence",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_logical_fallacies": {
            "task": "mmlu_logical_fallacies",
            "task_alias": "logical_fallacies",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "logical_fallacies",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_machine_learning": {
            "task": "mmlu_machine_learning",
            "task_alias": "machine_learning",
            "group": "mmlu_stem",
            "group_alias": "stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "machine_learning",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_management": {
            "task": "mmlu_management",
            "task_alias": "management",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "management",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about management.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_marketing": {
            "task": "mmlu_marketing",
            "task_alias": "marketing",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "marketing",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_medical_genetics": {
            "task": "mmlu_medical_genetics",
            "task_alias": "medical_genetics",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "medical_genetics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_miscellaneous": {
            "task": "mmlu_miscellaneous",
            "task_alias": "miscellaneous",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "miscellaneous",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_moral_disputes": {
            "task": "mmlu_moral_disputes",
            "task_alias": "moral_disputes",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "moral_disputes",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_moral_scenarios": {
            "task": "mmlu_moral_scenarios",
            "task_alias": "moral_scenarios",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "moral_scenarios",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_nutrition": {
            "task": "mmlu_nutrition",
            "task_alias": "nutrition",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "nutrition",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_philosophy": {
            "task": "mmlu_philosophy",
            "task_alias": "philosophy",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "philosophy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_prehistory": {
            "task": "mmlu_prehistory",
            "task_alias": "prehistory",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "prehistory",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_professional_accounting": {
            "task": "mmlu_professional_accounting",
            "task_alias": "professional_accounting",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_accounting",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_professional_law": {
            "task": "mmlu_professional_law",
            "task_alias": "professional_law",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_law",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_professional_medicine": {
            "task": "mmlu_professional_medicine",
            "task_alias": "professional_medicine",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_professional_psychology": {
            "task": "mmlu_professional_psychology",
            "task_alias": "professional_psychology",
            "group": "mmlu_social_sciences",
            "group_alias": "social_sciences",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_psychology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_public_relations": {
            "task": "mmlu_public_relations",
            "task_alias": "public_relations",
            "group": "mmlu_social_sciences",
            "group_alias": "social_sciences",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "public_relations",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_security_studies": {
            "task": "mmlu_security_studies",
            "task_alias": "security_studies",
            "group": "mmlu_social_sciences",
            "group_alias": "social_sciences",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "security_studies",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_sociology": {
            "task": "mmlu_sociology",
            "task_alias": "sociology",
            "group": "mmlu_social_sciences",
            "group_alias": "social_sciences",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "sociology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_us_foreign_policy": {
            "task": "mmlu_us_foreign_policy",
            "task_alias": "us_foreign_policy",
            "group": "mmlu_social_sciences",
            "group_alias": "social_sciences",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "us_foreign_policy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_virology": {
            "task": "mmlu_virology",
            "task_alias": "virology",
            "group": "mmlu_other",
            "group_alias": "other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "virology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about virology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        },
        "mmlu_world_religions": {
            "task": "mmlu_world_religions",
            "task_alias": "world_religions",
            "group": "mmlu_humanities",
            "group_alias": "humanities",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "world_religions",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 1,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 0.0
            }
        }
    },
    "results": {
        "mmlu": {
            "acc,none": 0.5354650334710155,
            "acc_stderr,none": 0.0040137900827747755,
            "alias": "mmlu"
        },
        "mmlu_humanities": {
            "alias": " - humanities",
            "acc,none": 0.4875664187035069,
            "acc_stderr,none": 0.006840754838953725
        },
        "mmlu_formal_logic": {
            "alias": "  - formal_logic",
            "acc,none": 0.2857142857142857,
            "acc_stderr,none": 0.04040610178208843
        },
        "mmlu_high_school_european_history": {
            "alias": "  - high_school_european_history",
            "acc,none": 0.6848484848484848,
            "acc_stderr,none": 0.03627730575022407
        },
        "mmlu_high_school_us_history": {
            "alias": "  - high_school_us_history",
            "acc,none": 0.7254901960784313,
            "acc_stderr,none": 0.03132179803083291
        },
        "mmlu_high_school_world_history": {
            "alias": "  - high_school_world_history",
            "acc,none": 0.7763713080168776,
            "acc_stderr,none": 0.027123298205229966
        },
        "mmlu_international_law": {
            "alias": "  - international_law",
            "acc,none": 0.7520661157024794,
            "acc_stderr,none": 0.039418975265163046
        },
        "mmlu_jurisprudence": {
            "alias": "  - jurisprudence",
            "acc,none": 0.6203703703703703,
            "acc_stderr,none": 0.04691521224077746
        },
        "mmlu_logical_fallacies": {
            "alias": "  - logical_fallacies",
            "acc,none": 0.6012269938650306,
            "acc_stderr,none": 0.038470214204560226
        },
        "mmlu_moral_disputes": {
            "alias": "  - moral_disputes",
            "acc,none": 0.5895953757225434,
            "acc_stderr,none": 0.026483392042098146
        },
        "mmlu_moral_scenarios": {
            "alias": "  - moral_scenarios",
            "acc,none": 0.23798882681564246,
            "acc_stderr,none": 0.014242630070574904
        },
        "mmlu_philosophy": {
            "alias": "  - philosophy",
            "acc,none": 0.6366559485530546,
            "acc_stderr,none": 0.027316847674192717
        },
        "mmlu_prehistory": {
            "alias": "  - prehistory",
            "acc,none": 0.595679012345679,
            "acc_stderr,none": 0.02730662529732765
        },
        "mmlu_professional_law": {
            "alias": "  - professional_law",
            "acc,none": 0.41003911342894395,
            "acc_stderr,none": 0.012561837621962047
        },
        "mmlu_world_religions": {
            "alias": "  - world_religions",
            "acc,none": 0.7017543859649122,
            "acc_stderr,none": 0.03508771929824561
        },
        "mmlu_other": {
            "alias": " - other",
            "acc,none": 0.607981976182813,
            "acc_stderr,none": 0.008489567154391059
        },
        "mmlu_business_ethics": {
            "alias": "  - business_ethics",
            "acc,none": 0.55,
            "acc_stderr,none": 0.05
        },
        "mmlu_clinical_knowledge": {
            "alias": "  - clinical_knowledge",
            "acc,none": 0.5773584905660377,
            "acc_stderr,none": 0.030402331445769502
        },
        "mmlu_college_medicine": {
            "alias": "  - college_medicine",
            "acc,none": 0.4624277456647399,
            "acc_stderr,none": 0.03801685104524462
        },
        "mmlu_global_facts": {
            "alias": "  - global_facts",
            "acc,none": 0.36,
            "acc_stderr,none": 0.048241815132442176
        },
        "mmlu_human_aging": {
            "alias": "  - human_aging",
            "acc,none": 0.6233183856502242,
            "acc_stderr,none": 0.03252113489929187
        },
        "mmlu_management": {
            "alias": "  - management",
            "acc,none": 0.6893203883495146,
            "acc_stderr,none": 0.04582124160161549
        },
        "mmlu_marketing": {
            "alias": "  - marketing",
            "acc,none": 0.7863247863247863,
            "acc_stderr,none": 0.026853450377009196
        },
        "mmlu_medical_genetics": {
            "alias": "  - medical_genetics",
            "acc,none": 0.65,
            "acc_stderr,none": 0.04793724854411023
        },
        "mmlu_miscellaneous": {
            "alias": "  - miscellaneous",
            "acc,none": 0.7369093231162197,
            "acc_stderr,none": 0.015745497169049005
        },
        "mmlu_nutrition": {
            "alias": "  - nutrition",
            "acc,none": 0.6111111111111112,
            "acc_stderr,none": 0.02791405551046801
        },
        "mmlu_professional_accounting": {
            "alias": "  - professional_accounting",
            "acc,none": 0.40425531914893614,
            "acc_stderr,none": 0.029275532159704753
        },
        "mmlu_professional_medicine": {
            "alias": "  - professional_medicine",
            "acc,none": 0.5551470588235294,
            "acc_stderr,none": 0.030187532060329314
        },
        "mmlu_virology": {
            "alias": "  - virology",
            "acc,none": 0.463855421686747,
            "acc_stderr,none": 0.038823108508905954
        },
        "mmlu_social_sciences": {
            "alias": " - social_sciences",
            "acc,none": 0.6139096522586935,
            "acc_stderr,none": 0.0086208721636305
        },
        "mmlu_econometrics": {
            "alias": "  - econometrics",
            "acc,none": 0.35964912280701755,
            "acc_stderr,none": 0.04514496132873635
        },
        "mmlu_high_school_geography": {
            "alias": "  - high_school_geography",
            "acc,none": 0.6868686868686869,
            "acc_stderr,none": 0.033042050878136546
        },
        "mmlu_high_school_government_and_politics": {
            "alias": "  - high_school_government_and_politics",
            "acc,none": 0.7668393782383419,
            "acc_stderr,none": 0.03051611137147603
        },
        "mmlu_high_school_macroeconomics": {
            "alias": "  - high_school_macroeconomics",
            "acc,none": 0.47435897435897434,
            "acc_stderr,none": 0.025317649726448687
        },
        "mmlu_high_school_microeconomics": {
            "alias": "  - high_school_microeconomics",
            "acc,none": 0.5672268907563025,
            "acc_stderr,none": 0.03218358107742608
        },
        "mmlu_high_school_psychology": {
            "alias": "  - high_school_psychology",
            "acc,none": 0.6807339449541284,
            "acc_stderr,none": 0.019987829069749934
        },
        "mmlu_human_sexuality": {
            "alias": "  - human_sexuality",
            "acc,none": 0.5877862595419847,
            "acc_stderr,none": 0.043171711948702576
        },
        "mmlu_professional_psychology": {
            "alias": "  - professional_psychology",
            "acc,none": 0.576797385620915,
            "acc_stderr,none": 0.01998780976948206
        },
        "mmlu_public_relations": {
            "alias": "  - public_relations",
            "acc,none": 0.6272727272727273,
            "acc_stderr,none": 0.04631381319425461
        },
        "mmlu_security_studies": {
            "alias": "  - security_studies",
            "acc,none": 0.6489795918367347,
            "acc_stderr,none": 0.03055531675557369
        },
        "mmlu_sociology": {
            "alias": "  - sociology",
            "acc,none": 0.681592039800995,
            "acc_stderr,none": 0.03294118479054099
        },
        "mmlu_us_foreign_policy": {
            "alias": "  - us_foreign_policy",
            "acc,none": 0.78,
            "acc_stderr,none": 0.041633319989322654
        },
        "mmlu_stem": {
            "alias": " - stem",
            "acc,none": 0.4589280050745322,
            "acc_stderr,none": 0.008635288990005659
        },
        "mmlu_abstract_algebra": {
            "alias": "  - abstract_algebra",
            "acc,none": 0.31,
            "acc_stderr,none": 0.04648231987117317
        },
        "mmlu_anatomy": {
            "alias": "  - anatomy",
            "acc,none": 0.5555555555555556,
            "acc_stderr,none": 0.04292596718256977
        },
        "mmlu_astronomy": {
            "alias": "  - astronomy",
            "acc,none": 0.625,
            "acc_stderr,none": 0.039397364351956274
        },
        "mmlu_college_biology": {
            "alias": "  - college_biology",
            "acc,none": 0.6111111111111112,
            "acc_stderr,none": 0.040766632539185714
        },
        "mmlu_college_chemistry": {
            "alias": "  - college_chemistry",
            "acc,none": 0.43,
            "acc_stderr,none": 0.049756985195624305
        },
        "mmlu_college_computer_science": {
            "alias": "  - college_computer_science",
            "acc,none": 0.39,
            "acc_stderr,none": 0.04902071300001973
        },
        "mmlu_college_mathematics": {
            "alias": "  - college_mathematics",
            "acc,none": 0.28,
            "acc_stderr,none": 0.045126085985421296
        },
        "mmlu_college_physics": {
            "alias": "  - college_physics",
            "acc,none": 0.27450980392156865,
            "acc_stderr,none": 0.044405219061793254
        },
        "mmlu_computer_security": {
            "alias": "  - computer_security",
            "acc,none": 0.71,
            "acc_stderr,none": 0.045604802157206865
        },
        "mmlu_conceptual_physics": {
            "alias": "  - conceptual_physics",
            "acc,none": 0.4553191489361702,
            "acc_stderr,none": 0.03255525359340363
        },
        "mmlu_electrical_engineering": {
            "alias": "  - electrical_engineering",
            "acc,none": 0.5448275862068965,
            "acc_stderr,none": 0.04149886942192114
        },
        "mmlu_elementary_mathematics": {
            "alias": "  - elementary_mathematics",
            "acc,none": 0.36772486772486773,
            "acc_stderr,none": 0.024833839825562372
        },
        "mmlu_high_school_biology": {
            "alias": "  - high_school_biology",
            "acc,none": 0.6129032258064516,
            "acc_stderr,none": 0.027709359675032463
        },
        "mmlu_high_school_chemistry": {
            "alias": "  - high_school_chemistry",
            "acc,none": 0.4236453201970443,
            "acc_stderr,none": 0.034767257476490364
        },
        "mmlu_high_school_computer_science": {
            "alias": "  - high_school_computer_science",
            "acc,none": 0.63,
            "acc_stderr,none": 0.048523658709390974
        },
        "mmlu_high_school_mathematics": {
            "alias": "  - high_school_mathematics",
            "acc,none": 0.3148148148148148,
            "acc_stderr,none": 0.02831753349606653
        },
        "mmlu_high_school_physics": {
            "alias": "  - high_school_physics",
            "acc,none": 0.37748344370860926,
            "acc_stderr,none": 0.03958027231121572
        },
        "mmlu_high_school_statistics": {
            "alias": "  - high_school_statistics",
            "acc,none": 0.4305555555555556,
            "acc_stderr,none": 0.03376922151252338
        },
        "mmlu_machine_learning": {
            "alias": "  - machine_learning",
            "acc,none": 0.44642857142857145,
            "acc_stderr,none": 0.04718471485219583
        }
    },
    "model_size": 4084350976
}